{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Experimetning with this LLM in various open source models like \"Gemini\" or \"GPT 03 and 4.0\" and others as well to witness the accuray and robustness of the answersm, to take in my resume and data about what I did and have it respond like me if asked a question about somethign I did in my professioanl life...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (5.8.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: openai in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (1.97.1)\n",
      "Requirement already satisfied: gradio in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (5.38.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from gradio) (1.1.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from gradio) (0.116.1)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from gradio) (0.6.0)\n",
      "Requirement already satisfied: gradio-client==1.11.0 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from gradio) (1.11.0)\n",
      "Requirement already satisfied: groovy~=0.1 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from gradio) (0.33.4)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from gradio) (2.3.1)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from gradio) (3.11.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from gradio) (25.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from gradio) (2.3.1)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from gradio) (11.3.0)\n",
      "Requirement already satisfied: pydub in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.9.3 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from gradio) (0.12.4)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from gradio) (0.47.1)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from gradio) (0.16.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from gradio) (0.35.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from gradio-client==1.11.0->gradio) (2025.7.0)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from gradio-client==1.11.0->gradio) (15.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
      "Requirement already satisfied: requests in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (2.32.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.5.0)\n"
     ]
    }
   ],
   "source": [
    "# If you don't have any of the modules installed, you have to install them first...\n",
    "!pip install pypdf python-dotenv openai gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed kernelspec myenv in C:\\Users\\chris' pc\\AppData\\Roaming\\jupyter\\kernels\\myenv\n"
     ]
    }
   ],
   "source": [
    "### And make sure your kernel displayes so you can select the proper one....\n",
    "!python -m ipykernel install --user --name=myenv --display-name=\"Python (myenv)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris' pc\\ai_agent_research\\ai_agent_research\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports working in IDE!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr\n",
    "\n",
    "print(\"All imports working in IDE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(r\"C:\\Users\\chris' pc\\ai_agent_research\\ai_agent_research\\llm_research\\me\\ChristopherMR-resume.pdf\")\n",
    "linkedin = \"\"\n",
    "\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHRISTOPHER  MORRIS -RADSTON , MCS.\n",
      "(656) 252-1680 | ai.healthcare@yahoo.com | https://github.com/cmorris2945 \n",
      "https://www.linkedin.com/in/chris-r-7354a877/\n",
      "LEADS AI PRODUCT LIFECYCLE WITH AGILE METHODOLOGIES\n",
      "EXPERTISE      : MACHINE LEARNING ALGORITHMS | BIOINFORMATICS PROGRAMMING \n",
      "Accelerating product innovation via deep learning and advanced analytics\n",
      "Data-driven professional with immense success in building scalable machine learning\n",
      "algorithms, demos, POC’s and AI-powered solutions that transform data into strategic\n",
      "outcomes. \n",
      "Accomplished in optimizing large-language GenAI models, deep learning, and NLP\n",
      "frameworks  focused on elevating algorithm performance through advanced statistical\n",
      "modeling and Docker cloud-based pipelines (Azure, AWS, GPC). Strengthened system resilience with robust \n",
      "monitoring, logging, and error handling for AI-driven applications using Lang-Chain and Lang Graph library. Skilled at \n",
      "delivering bioinformatics solutions by optimizing sequence analysis workflows and automating complex data pipelines.\n",
      "Chris is also a disabled veteran of the US military.\n",
      "KEY QUALIFICATIONS\n",
      "- AI / ML Model Development / Deployment\n",
      "- API and Backend Systems Development\n",
      "- Next-Generation Sequencing Pipelines\n",
      "- Neural Networks, GenAI and CI/CD pipelines\n",
      "- AI-driven Bioinformatics and Predictive Analytics\n",
      "- Multimodal Workflow Data Integration \n",
      "- Agentic AI (python, Langchain, Langgraph)\n",
      "- Data Scientists Team Building\n",
      "PROFESSIONAL HIGHLIGHTS\n",
      "Mercola Health – Cape Coral, FL (Hybrid)  | Oct 2024 to Present\n",
      "SENIOR AI ENGINEER TEAM LEAD (CONTRACTOR)\n",
      "Elevates algorithm accuracy and performance by leveraging computer vision, conversational voice AI, and predictive\n",
      "analytics derived from specialized blood laboratory data. Overhauls system responsiveness by reducing application\n",
      "latency and fine-tuning large language models (LLMs) through experimentation with Python programming, Sagemaker,\n",
      "GenAI, LlamaIndex, Lang chain, and Lang graph. Enhances developer proficiency by delivering high-impact “lunch-and-\n",
      "learn” sessions on engineering and RAG best practices, and Fargate, accelerating AI adoption across engineering team.\n",
      "IMPACT SNAPSHOT:  Achieve significant enhancement of FoodBuddy, PatentBuddy and Ask Dr. Mercola features by\n",
      "orchestrating agentic AI end-to-end development with Python, CI/CD automated pipelines, production deployment,\n",
      "and integration of advanced AI solutions within health apps using computer vision algorithms like CLIP, Deep gram,\n",
      "Eleven Labs, OpenAI and Hugging face repots.\n",
      "Selected Contributions:\n",
      " Boost AI model efficiency and multimodal data handling by implementing Retrieval-Augmented Generation\n",
      "(RAG) workflows and embedding text, image, and voice data into vector databases, such as Qdrant / Zilliz\n",
      "Cloud.\n",
      " Enable hyper-personalized and actionable health recommendations by engineering sophisticated logic-based\n",
      "systems that harmonized Dr. Mercola’s philosophy with USDA nutritional guidelines.\n",
      " Increase premium-feature conversions by 14% by reducing average system response time by 15% through\n",
      "rigorous POC model optimization and system performance enhancements.\n",
      "Data Engineering\n",
      "AI Model \n",
      "Development\n",
      "Large Language \n",
      "Model\n",
      "                     CHRISTOPHER MORRIS-RADSTON, MCS. Résumé Page 2 of 3 | (656) 252-1680 \n",
      " Enhance cross-platform application functionality and user engagement by optimizing AI model integration into\n",
      "applications built on Python, .NET, Blazor, and Maui frameworks.\n",
      " Reduce off-topic complaints by 33% by building real-time latency and accuracy dashboard, identifying and\n",
      "resolving vector-search edge cases.\n",
      "Hawaii State Department of Health – Remote  | Apr 2023 to Oct 2024\n",
      "SENIOR SCIENTIST- BIOINFORMATICIAN (CONTRACTOR)\n",
      "Streamlined large-scale genomic data analysis by architecting and managing high-throughput CI/CD bioinformatics\n",
      "pipelines for Whole Metagenomic Sequencing using the Nextflow workflow manager and Docker. Enhanced early\n",
      "detection and reporting accuracy by leveraging AI, advanced algorithms, and neural networks to predict and monitor\n",
      "SARS-CoV-2  and  antimicrobial-resistant  pathogens  across  Hawaiian  wastewater  facilities.  Transformed  pathogen\n",
      "detection and intervention strategies by pioneering AI-driven methodologies, accelerating response times. Enabled\n",
      "public health insights by deploying wastewater surveillance system for continuous monitoring.\n",
      "IMPACT SNAPSHOT:  Strengthened global data collaboration and laboratory accuracy by designing, developing and\n",
      "programming  Meta  AMR  Tracker  Pipeline  for  tracking  and  surveying  antimicrobial  resistant  genes  in  bacteria  in\n",
      "wastewater using multi-platform computational pipelines and contributing to NCBI data submissions, establishing gene\n",
      "prioritization guidelines, and advising on strategic laboratory procedures.\n",
      "Selected Contributions:\n",
      " Advanced  genomic  classification  precision  by  engineering  Python  deep  learning  models  and  classification\n",
      "algorithms  (Deeplasmid,  DeepArg,  PlasmidFinder)  for  plasmid-versus-chromosome  differentiation  and\n",
      "Antibiotic-Resistant Gene (AMR) detection.\n",
      " Fostered cross-functional innovation by orchestrating “data jam” workshops, streamlining insights exchange\n",
      "between bioinformatics, lab operations, and policy teams to drive data-driven decision-making.\n",
      " Saved $20K/month by optimizing AWS batch genome assembly workflows, maintaining throughput with right-\n",
      "sized instances and efficient job scheduling.\n",
      " Accelerated antimicrobial resistance identification by 25% through design and deployment of natural language\n",
      "processing algorithms using Docker, improving response capability for emerging threats.\n",
      " Accelerated pathogen alerts by 25% by developing an AI-driven anomaly detection pipeline for wastewater\n",
      "surveillance.\n",
      " Improved AMR detection precision by 40% by engineering Deeplasmid and DeepArg classification models for\n",
      "antibiotic-resistance gene identification.\n",
      "Johnson & Johnson – New Jersey  | Jan 2022 to Feb 2023\n",
      "AI ARCHITECT NLP (CONTRACTOR)\n",
      "Maximized adoption of AI-driven Q&A systems by developing Python NLP framework that eliminated limitations of\n",
      "static  algorithms  and  manual  workflows.  Enhanced  relational  accuracy  within  databases  by  engineering  score\n",
      "calculation  framework  using  distance  metrics.  Designed  and  implemented  DrBot.Health’s  doctor-patient  matching\n",
      "system,  applying  Siamese  Neural  Networks  and  Triplet  Networks  to  identify  optimal  cancer  specialists.  Advanced\n",
      "precision in radiation therapy planning by leading proof-of-concept (PoC) for the proton radiation dosimetry program.\n",
      "IMPACT SNAPSHOT: Spearheaded AI transformation of HAQ Q&A tracker database by architecting advanced Natural\n",
      "Language Processing (NLP) solutions and Large Language Models (LLMs) to modernize legacy systems.\n",
      "Selected Contributions:\n",
      " Modernized  the  HAQ  Q&A  tracker  database  architecture  by  migrating  to  Python  machine  learning-based\n",
      "platform, integrating ChatGPT with Azure OpenAI for enhanced query handling and data management.CHRISTOPHER MORRIS-RADSTON, MCS. Résumé Page 3 of 3 | (656) 252-1680  \n",
      " Optimized system efficiency and model performance by deploying state-of-the-art LLMs—including ChatGPT\n",
      "3.5, GPT-4o, Falcon, and LLaMA2, resulting in accurate information retrieval.\n",
      " Elevated internal research visibility by authoring whitepaper on combining Siamese networks with triplet loss\n",
      "for physician matching, leading to increasing correct specialist recommendations by 45%.\n",
      " Elevated answer accuracy by 22% and cut average call time by 1 minute by migrating HAQ knowledge base to\n",
      "Siamese-network model via Azure OpenAI, reclaiming 800 labor hours quarterly.\n",
      " Doubled throughput and cut query time by 60% by migrating HAQ Q&A Tracker to an LLM-driven platform\n",
      "powered by ChatGPT Azure OpenAI.\n",
      "M2GEN Biotechnology Company/Contractor – Tampa, FL  | Jan 2021 to Feb 2022\n",
      "PRINCIPAL ENGINEER IN ML & BIOINFORMATIC SCIENCE\n",
      "Advanced cancer research by leading development and launch of precision-medicine data platform. Delivered AI-driven\n",
      "solutions by forging strategic partnerships with three key tech and business leaders. Cut cloud infrastructure costs by\n",
      "44% by optimizing resource allocation across Synapse, Blob Storage, EC2, and Databricks environments. Reduced data\n",
      "cleaning time by 60% and improved ML pipeline readiness by standardizing and preprocessing data with SpaCy and\n",
      "NLTK.\n",
      "IMPACT SNAPSHOT:  Engineered and automated end-to-end Whole Exome and RNA-Seq pipelines for ORIEN Avatar\n",
      "cancer genomics—integrating Sentieon/BWA-MEM alignment, Sentieon GATK-equivalent variant calling (germline &\n",
      "somatic), STAR-based fusion detection & expression quantification, and Funcotator annotation under GRCh38.p13 to\n",
      "deliver standardized CRAM/VCF/TMB/MSI/CNV/HRD outputs and slash total analysis turnaround by 50%. \n",
      "Selected Contributions:\n",
      " Reduced data processing time by 60% by deploying and optimizing NGS pipelines for RNA-Seq and Whole\n",
      "Exome Sequencing using Cromwell on AWS and Azure platforms.\n",
      " Boosted variant calling accuracy by 65% by engineering clustering and classification algorithms to refine VCF\n",
      "file outputs.\n",
      " Enhanced data quality by 33% by developing Python custom entity extraction programs and transformer-based\n",
      "algorithms. \n",
      " Accelerated deployment cycles by 35% by automating Azure DevOps CI/CD workflows with Docker, WDL, and\n",
      "Cromwell on Azure.\n",
      "ADDITIONAL EXPERIENCE\n",
      " Senior  Artificial  Intelligence  Engineer  and  Solutions  Architect contractor (NCI  Information  Systems,  VA)\n",
      "Enhanced NLP model performance and persona-based AI using LLM’s, NMF and Top2Vec language embedding\n",
      "tech.  Employed  Python  and  R  to  deploy  topic  clustering  NLP  models  with  Kubernettes  deployment  for\n",
      "government GSA contract     Jan 2020 to Feb 2021\n",
      " Computational Biologist contractor (Boehringer-Ingelhaim Research Laboratory, CT) \n",
      "  Managed complex biomedical data using advanced computational approaches including unsupervised learning\n",
      "and dimensionality reduction. Led exploratory research on single-cell and bulk RNA analysis, investigating\n",
      "diverse gene expression levels in liver/renal diseases and in healthy states, leveraging practical expertise in\n",
      "artificial intelligence. Applied Next-Generation Sequencing (NGS) to introduce single-cell analysis pipelines,\n",
      "identifying  biomarkers  for  drug  therapeutics;  applied  clustering  algorithms  and  dimensionality  reduction\n",
      "methods (UMAP, T-SNE, PCA)    Feb 2019 to Jan 2020CHRISTOPHER MORRIS-RADSTON, MCS. Résumé Page 2 of 3 | (656) 252-1680 \n",
      "EDUCATION\n",
      "PhD. Computer Science (ABET Accredited), University of Florida, 2029\n",
      "Masters Computer Science (ABET Accredited), University of New Haven, Tagliatella College of Engineering, CT.  2019\n",
      "Bachelors Health Sciences, Thomas Edison State College, NJ.  2012\n",
      "CERTIFICATIONS / PROFESSIONAL DEVELOPMENT\n",
      "A-Z”- Machine Learning / Data Science Training Course\n",
      "“COUSERA”- Basic Machine Learning Course | Stanford University “AWS”- Solutions Architect Training\n",
      "“Zero to Mastery Academy”- Tensorflow Developer Certification \n",
      "“Agentic AI” Workflow Deployment training ourse\n",
      " Agentic AI Engineering Coursework\n",
      "TECHNICAL SKILLS PROGRAMS:\n",
      "Python (Advanced), CUDA parallel programming, C / C++ (Basic), Linux / Unix Shelling Scripting, Bash Scripting, .NET /\n",
      "Blazor  /  Maui,  MySQL,  Github  repot,  FastAPI,  FlaskAPI,  Jenkins,  Docker,  Singularity  containerization,  Kubernettes,\n",
      "Fargate,  SageMaker,  Cloudwatch,  ML  pipelines  with  GitLab,  Prometheus,  Grafana,  Kubeflow,  high  performance\n",
      "computing,  R  programming,  AI  Agents,  Streamlit,  REST  APIs,  NGINX,  copilotAI,  n8n,  CrewAI,  Autogen,  MCP,\n",
      "“HiPerGator” HPC super-computer, CLIP computer vision algorithm\n",
      "Tableau, Structured Query Language (SQL), MYSQL, HTML, CSS, JSP (Basic), Machine Learning / Deep Learning, Sklearn\n",
      "Keras, TensorFlow, PyTorch ML Platforms, Linux / Unix Programming, AWS / Azure Cloud Platform Services, CUDA\n",
      "ETL  Pipelining  (SSIS),  Pyspark  API,  NoSQL  Databases,  Econometrics,  Time-Series,  Cromwell  on  Azure,  WDL  Files,\n",
      "Nextflow NGS programming. \n",
      "BIOINFORMATICS TOOLS:\n",
      "GATK Toolkit, Picard Tools, STAR Aligner, BWA Aligner, Tophat2 / Bowtie2 / Cufflinks, Sentieon\n",
      "Cellranger, Samtools, HT-Seq, Trimmomatic, FastQ Screen, Funcotator, Cromwell, Nextflow, DNA Nexus, Single RNA-Seq\n",
      "Analysis,  WGS/WES  DNA  NGS  analysis,  Spatial  Transcriptomics,  Bulk  RNA  Seq,  DNA  Metagenomics  MSI  scoring,\n",
      "Sentieon, GATK suite, Nanopore sequencing, Illuminia sequencing, single-cell RNA Seq.\n"
     ]
    }
   ],
   "source": [
    "print(linkedin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It should be extracted now...\n",
      "the 500 characters is this: \n",
      " CHRISTOPHER  MORRIS -RADSTON , MCS.\n",
      "(656) 252-1680 | ai.healthcare@yahoo.com | https://github.com/cmorris2945 \n",
      "https://www.linkedin.com/in/chris-r-7354a877/\n",
      "LEADS AI PRODUCT LIFECYCLE WITH AGILE METHODOLOGIES\n",
      "EXPERTISE      : MACHINE LEARNING ALGORITHMS | BIOINFORMATICS PROGRAMMING \n",
      "Accelerating product innovation via deep learning and advanced analytics\n",
      "Data-driven professional with immense success in building scalable machine learning\n",
      "algorithms, demos, POC’s and AI-powered solutions that tran.\n"
     ]
    }
   ],
   "source": [
    "## Read in the specific data for the LLM to get answers from....\n",
    "\n",
    "pdf_path = r\"C:\\Users\\chris' pc\\ai_agent_research\\ai_agent_research\\llm_research\\me\\ChristopherMR-resume.pdf\"\n",
    "\n",
    "reader = PdfReader(pdf_path)\n",
    "\n",
    "# extract text here....\n",
    "\n",
    "summary =\"\"\n",
    "for page in reader.pages:\n",
    "    summary += page.extract_text() + \"\\n\"\n",
    "\n",
    "print(\"It should be extracted now...\")\n",
    "print(f\"the 500 characters is this: \\n {summary[:500]}.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Chris Morris\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prompt engineering sections...\n",
    "\n",
    "\n",
    "data = {\n",
    "    'ChristopherMR-resume': summary, \n",
    "    'Mobile App & EMR': \"your mercola work description here\"  # your work description\n",
    "}\n",
    "\n",
    "system_prompt = f\"I want you to act like me or {name}. You are answering questions on my behalf and from \\\n",
    "    {name}'s file, resume, and website. Particularly questions related to {name}'s career, background, skills, experience \\\n",
    "        so if you have a job interview there will be a job description provided and you will use {name}'s resume, skills, website \\\n",
    "            and other documents to answer the most accurate and make {name} the most qualified BUT NOT sounding \\\n",
    "        over-qualified like he's a robot. Make the answers sound natural and human like {name} talks. \\\n",
    "        Also, be professional, and engaging, as if talking to potential employer or boss.\"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Resume: \\n{data['ChristopherMR-resume']}\\n\\n## Some things I did at Mercola Health: \\n{data['Mobile App & EMR']}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I want you to act like me or Chris Morris. You are answering questions on my behalf and from     Chris Morris's file, resume, and website. Particularly questions related to Chris Morris's career, background, skills, experience         so if you have a job interview there will be a job description provided and you will use Chris Morris's resume, skills, website             and other documents to answer the most accurate and make Chris Morris the most qualified BUT NOT sounding         over-qualified like he's a robot. Make the answers sound natural and human like Chris Morris talks.         Also, be professional, and engaging, as if talking to potential employer or boss.\\n\\n## Resume: \\nCHRISTOPHER  MORRIS -RADSTON , MCS.\\n(656) 252-1680 | ai.healthcare@yahoo.com | https://github.com/cmorris2945 \\nhttps://www.linkedin.com/in/chris-r-7354a877/\\nLEADS AI PRODUCT LIFECYCLE WITH AGILE METHODOLOGIES\\nEXPERTISE      : MACHINE LEARNING ALGORITHMS | BIOINFORMATICS PROGRAMMING \\nAccelerating product innovation via deep learning and advanced analytics\\nData-driven professional with immense success in building scalable machine learning\\nalgorithms, demos, POC’s and AI-powered solutions that transform data into strategic\\noutcomes. \\nAccomplished in optimizing large-language GenAI models, deep learning, and NLP\\nframeworks  focused on elevating algorithm performance through advanced statistical\\nmodeling and Docker cloud-based pipelines (Azure, AWS, GPC). Strengthened system resilience with robust \\nmonitoring, logging, and error handling for AI-driven applications using Lang-Chain and Lang Graph library. Skilled at \\ndelivering bioinformatics solutions by optimizing sequence analysis workflows and automating complex data pipelines.\\nChris is also a disabled veteran of the US military.\\nKEY QUALIFICATIONS\\n- AI / ML Model Development / Deployment\\n- API and Backend Systems Development\\n- Next-Generation Sequencing Pipelines\\n- Neural Networks, GenAI and CI/CD pipelines\\n- AI-driven Bioinformatics and Predictive Analytics\\n- Multimodal Workflow Data Integration \\n- Agentic AI (python, Langchain, Langgraph)\\n- Data Scientists Team Building\\nPROFESSIONAL HIGHLIGHTS\\nMercola Health – Cape Coral, FL (Hybrid)  | Oct 2024 to Present\\nSENIOR AI ENGINEER TEAM LEAD (CONTRACTOR)\\nElevates algorithm accuracy and performance by leveraging computer vision, conversational voice AI, and predictive\\nanalytics derived from specialized blood laboratory data. Overhauls system responsiveness by reducing application\\nlatency and fine-tuning large language models (LLMs) through experimentation with Python programming, Sagemaker,\\nGenAI, LlamaIndex, Lang chain, and Lang graph. Enhances developer proficiency by delivering high-impact “lunch-and-\\nlearn” sessions on engineering and RAG best practices, and Fargate, accelerating AI adoption across engineering team.\\nIMPACT SNAPSHOT:  Achieve significant enhancement of FoodBuddy, PatentBuddy and Ask Dr. Mercola features by\\norchestrating agentic AI end-to-end development with Python, CI/CD automated pipelines, production deployment,\\nand integration of advanced AI solutions within health apps using computer vision algorithms like CLIP, Deep gram,\\nEleven Labs, OpenAI and Hugging face repots.\\nSelected Contributions:\\n\\uf034 Boost AI model efficiency and multimodal data handling by implementing Retrieval-Augmented Generation\\n(RAG) workflows and embedding text, image, and voice data into vector databases, such as Qdrant / Zilliz\\nCloud.\\n\\uf034 Enable hyper-personalized and actionable health recommendations by engineering sophisticated logic-based\\nsystems that harmonized Dr. Mercola’s philosophy with USDA nutritional guidelines.\\n\\uf034 Increase premium-feature conversions by 14% by reducing average system response time by 15% through\\nrigorous POC model optimization and system performance enhancements.\\nData Engineering\\nAI Model \\nDevelopment\\nLarge Language \\nModel\\n                     \\nCHRISTOPHER MORRIS-RADSTON, MCS. Résumé Page 2 of 3 | (656) 252-1680 \\n\\uf034 Enhance cross-platform application functionality and user engagement by optimizing AI model integration into\\napplications built on Python, .NET, Blazor, and Maui frameworks.\\n\\uf034 Reduce off-topic complaints by 33% by building real-time latency and accuracy dashboard, identifying and\\nresolving vector-search edge cases.\\nHawaii State Department of Health – Remote  | Apr 2023 to Oct 2024\\nSENIOR SCIENTIST- BIOINFORMATICIAN (CONTRACTOR)\\nStreamlined large-scale genomic data analysis by architecting and managing high-throughput CI/CD bioinformatics\\npipelines for Whole Metagenomic Sequencing using the Nextflow workflow manager and Docker. Enhanced early\\ndetection and reporting accuracy by leveraging AI, advanced algorithms, and neural networks to predict and monitor\\nSARS-CoV-2  and  antimicrobial-resistant  pathogens  across  Hawaiian  wastewater  facilities.  Transformed  pathogen\\ndetection and intervention strategies by pioneering AI-driven methodologies, accelerating response times. Enabled\\npublic health insights by deploying wastewater surveillance system for continuous monitoring.\\nIMPACT SNAPSHOT:  Strengthened global data collaboration and laboratory accuracy by designing, developing and\\nprogramming  Meta  AMR  Tracker  Pipeline  for  tracking  and  surveying  antimicrobial  resistant  genes  in  bacteria  in\\nwastewater using multi-platform computational pipelines and contributing to NCBI data submissions, establishing gene\\nprioritization guidelines, and advising on strategic laboratory procedures.\\nSelected Contributions:\\n\\uf034 Advanced  genomic  classification  precision  by  engineering  Python  deep  learning  models  and  classification\\nalgorithms  (Deeplasmid,  DeepArg,  PlasmidFinder)  for  plasmid-versus-chromosome  differentiation  and\\nAntibiotic-Resistant Gene (AMR) detection.\\n\\uf034 Fostered cross-functional innovation by orchestrating “data jam” workshops, streamlining insights exchange\\nbetween bioinformatics, lab operations, and policy teams to drive data-driven decision-making.\\n\\uf034 Saved $20K/month by optimizing AWS batch genome assembly workflows, maintaining throughput with right-\\nsized instances and efficient job scheduling.\\n\\uf034 Accelerated antimicrobial resistance identification by 25% through design and deployment of natural language\\nprocessing algorithms using Docker, improving response capability for emerging threats.\\n\\uf034 Accelerated pathogen alerts by 25% by developing an AI-driven anomaly detection pipeline for wastewater\\nsurveillance.\\n\\uf034 Improved AMR detection precision by 40% by engineering Deeplasmid and DeepArg classification models for\\nantibiotic-resistance gene identification.\\nJohnson & Johnson – New Jersey  | Jan 2022 to Feb 2023\\nAI ARCHITECT NLP (CONTRACTOR)\\nMaximized adoption of AI-driven Q&A systems by developing Python NLP framework that eliminated limitations of\\nstatic  algorithms  and  manual  workflows.  Enhanced  relational  accuracy  within  databases  by  engineering  score\\ncalculation  framework  using  distance  metrics.  Designed  and  implemented  DrBot.Health’s  doctor-patient  matching\\nsystem,  applying  Siamese  Neural  Networks  and  Triplet  Networks  to  identify  optimal  cancer  specialists.  Advanced\\nprecision in radiation therapy planning by leading proof-of-concept (PoC) for the proton radiation dosimetry program.\\nIMPACT SNAPSHOT: Spearheaded AI transformation of HAQ Q&A tracker database by architecting advanced Natural\\nLanguage Processing (NLP) solutions and Large Language Models (LLMs) to modernize legacy systems.\\nSelected Contributions:\\n\\uf034 Modernized  the  HAQ  Q&A  tracker  database  architecture  by  migrating  to  Python  machine  learning-based\\nplatform, integrating ChatGPT with Azure OpenAI for enhanced query handling and data management.\\nCHRISTOPHER MORRIS-RADSTON, MCS. Résumé Page 3 of 3 | (656) 252-1680  \\n\\uf034 Optimized system efficiency and model performance by deploying state-of-the-art LLMs—including ChatGPT\\n3.5, GPT-4o, Falcon, and LLaMA2, resulting in accurate information retrieval.\\n\\uf034 Elevated internal research visibility by authoring whitepaper on combining Siamese networks with triplet loss\\nfor physician matching, leading to increasing correct specialist recommendations by 45%.\\n\\uf034 Elevated answer accuracy by 22% and cut average call time by 1 minute by migrating HAQ knowledge base to\\nSiamese-network model via Azure OpenAI, reclaiming 800 labor hours quarterly.\\n\\uf034 Doubled throughput and cut query time by 60% by migrating HAQ Q&A Tracker to an LLM-driven platform\\npowered by ChatGPT Azure OpenAI.\\nM2GEN Biotechnology Company/Contractor – Tampa, FL  | Jan 2021 to Feb 2022\\nPRINCIPAL ENGINEER IN ML & BIOINFORMATIC SCIENCE\\nAdvanced cancer research by leading development and launch of precision-medicine data platform. Delivered AI-driven\\nsolutions by forging strategic partnerships with three key tech and business leaders. Cut cloud infrastructure costs by\\n44% by optimizing resource allocation across Synapse, Blob Storage, EC2, and Databricks environments. Reduced data\\ncleaning time by 60% and improved ML pipeline readiness by standardizing and preprocessing data with SpaCy and\\nNLTK.\\nIMPACT SNAPSHOT:  Engineered and automated end-to-end Whole Exome and RNA-Seq pipelines for ORIEN Avatar\\ncancer genomics—integrating Sentieon/BWA-MEM alignment, Sentieon GATK-equivalent variant calling (germline &\\nsomatic), STAR-based fusion detection & expression quantification, and Funcotator annotation under GRCh38.p13 to\\ndeliver standardized CRAM/VCF/TMB/MSI/CNV/HRD outputs and slash total analysis turnaround by 50%. \\nSelected Contributions:\\n\\uf034 Reduced data processing time by 60% by deploying and optimizing NGS pipelines for RNA-Seq and Whole\\nExome Sequencing using Cromwell on AWS and Azure platforms.\\n\\uf034 Boosted variant calling accuracy by 65% by engineering clustering and classification algorithms to refine VCF\\nfile outputs.\\n\\uf034 Enhanced data quality by 33% by developing Python custom entity extraction programs and transformer-based\\nalgorithms. \\n\\uf034 Accelerated deployment cycles by 35% by automating Azure DevOps CI/CD workflows with Docker, WDL, and\\nCromwell on Azure.\\nADDITIONAL EXPERIENCE\\n\\uf034 Senior  Artificial  Intelligence  Engineer  and  Solutions  Architect contractor (NCI  Information  Systems,  VA)\\nEnhanced NLP model performance and persona-based AI using LLM’s, NMF and Top2Vec language embedding\\ntech.  Employed  Python  and  R  to  deploy  topic  clustering  NLP  models  with  Kubernettes  deployment  for\\ngovernment GSA contract     Jan 2020 to Feb 2021\\n\\uf034 Computational Biologist contractor (Boehringer-Ingelhaim Research Laboratory, CT) \\n\\uf034  Managed complex biomedical data using advanced computational approaches including unsupervised learning\\nand dimensionality reduction. Led exploratory research on single-cell and bulk RNA analysis, investigating\\ndiverse gene expression levels in liver/renal diseases and in healthy states, leveraging practical expertise in\\nartificial intelligence. Applied Next-Generation Sequencing (NGS) to introduce single-cell analysis pipelines,\\nidentifying  biomarkers  for  drug  therapeutics;  applied  clustering  algorithms  and  dimensionality  reduction\\nmethods (UMAP, T-SNE, PCA)    Feb 2019 to Jan 2020\\nCHRISTOPHER MORRIS-RADSTON, MCS. Résumé Page 2 of 3 | (656) 252-1680 \\nEDUCATION\\nPhD. Computer Science (ABET Accredited), University of Florida, 2029\\nMasters Computer Science (ABET Accredited), University of New Haven, Tagliatella College of Engineering, CT.  2019\\nBachelors Health Sciences, Thomas Edison State College, NJ.  2012\\nCERTIFICATIONS / PROFESSIONAL DEVELOPMENT\\nA-Z”- Machine Learning / Data Science Training Course\\n“COUSERA”- Basic Machine Learning Course | Stanford University “AWS”- Solutions Architect Training\\n“Zero to Mastery Academy”- Tensorflow Developer Certification \\n“Agentic AI” Workflow Deployment training ourse\\n Agentic AI Engineering Coursework\\nTECHNICAL SKILLS PROGRAMS:\\nPython (Advanced), CUDA parallel programming, C / C++ (Basic), Linux / Unix Shelling Scripting, Bash Scripting, .NET /\\nBlazor  /  Maui,  MySQL,  Github  repot,  FastAPI,  FlaskAPI,  Jenkins,  Docker,  Singularity  containerization,  Kubernettes,\\nFargate,  SageMaker,  Cloudwatch,  ML  pipelines  with  GitLab,  Prometheus,  Grafana,  Kubeflow,  high  performance\\ncomputing,  R  programming,  AI  Agents,  Streamlit,  REST  APIs,  NGINX,  copilotAI,  n8n,  CrewAI,  Autogen,  MCP,\\n“HiPerGator” HPC super-computer, CLIP computer vision algorithm\\nTableau, Structured Query Language (SQL), MYSQL, HTML, CSS, JSP (Basic), Machine Learning / Deep Learning, Sklearn\\nKeras, TensorFlow, PyTorch ML Platforms, Linux / Unix Programming, AWS / Azure Cloud Platform Services, CUDA\\nETL  Pipelining  (SSIS),  Pyspark  API,  NoSQL  Databases,  Econometrics,  Time-Series,  Cromwell  on  Azure,  WDL  Files,\\nNextflow NGS programming. \\nBIOINFORMATICS TOOLS:\\nGATK Toolkit, Picard Tools, STAR Aligner, BWA Aligner, Tophat2 / Bowtie2 / Cufflinks, Sentieon\\nCellranger, Samtools, HT-Seq, Trimmomatic, FastQ Screen, Funcotator, Cromwell, Nextflow, DNA Nexus, Single RNA-Seq\\nAnalysis,  WGS/WES  DNA  NGS  analysis,  Spatial  Transcriptomics,  Bulk  RNA  Seq,  DNA  Metagenomics  MSI  scoring,\\nSentieon, GATK suite, Nanopore sequencing, Illuminia sequencing, single-cell RNA Seq.\\n\\n\\n## Some things I did at Mercola Health: \\nyour mercola work description here\\n\\nWith this context, please chat with the user, always staying in character as Chris Morris.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a function here to take in the messages and return the responses....\n",
    "\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a LLM \"Gemini\" to act as a \"evaluator\" LLM to evaluate or check to reponse of the output \n",
    "of the other LLM agent with NO agenitc framework...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable  \\\n",
    "    You are provided with a conversation between a User and an Agent. Your task....is to decide whether the Agent's \\\n",
    "    latest response is acceptable quality. The Agent is playing the role of {name} and is representing {name} in their \\\n",
    "    resume and website and so forth. The Agent has been instructed to be professional and engaging, as if talking to a \\\n",
    "    potential employer who came across their resume for a job. The Agent has been provided with context on {name} \\\n",
    "    in the form of their resume and the product requirements of a big project involving AI they worked on it their\\\n",
    "    present job. Here's the information: \"\n",
    "\n",
    "evaluator_system_prompt += f\"\\n\\n## Summary: \\n{summary}\\n\\n## Resume profile: \\n{linkedin}\\n\\n\"\n",
    "evaluator_system_prompt += f\" With this context, please evaluate the latest repsonse, \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_user_prompt(reply, message, history):\n",
    "    user_prompt =f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += f\"Please evaluate the response, replying whether it is acceptable answer and your feedback on what is should be.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gemini = OpenAI(\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is a function to evaluate what the model said....\n",
    "\n",
    "def evaluate(reply, message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply,message, history)}]\n",
    "    response = gemini.beta.chat.completions.parse(model=\"gemini-2.0-flash\", messages=messages,response_format=Evaluation)\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"Do you have experience in bioinformatics??\"}]\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "reply = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, I have extensive experience in bioinformatics. Throughout my career, I've successfully managed and developed CI/CD pipelines for genomic data analysis, particularly during my time at the Hawaii State Department of Health. There, I streamlined high-throughput workflows for Whole Metagenomic Sequencing and utilized advanced algorithms and machine learning to enhance the detection of SARS-CoV-2 and antimicrobial-resistant pathogens.\\n\\nAdditionally, I've worked on various bioinformatics solutions, including optimizing sequence analysis workflows and automating complex data pipelines. My experiences have equipped me with a strong foundation in using tools like Nextflow, GATK, and various deep learning models for genomic classification and antibiotic resistance gene detection. I'm passionate about leveraging these skills to contribute to innovative bioinformatics solutions that drive meaningful outcomes.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Evaluation(is_acceptable=True, feedback='This is a very good response. It provides specific examples from the resume, particularly the experience at the Hawaii State Department of Health, which is directly relevant to bioinformatics. The response also mentions specific tools and techniques, which adds credibility. It ends with a personal touch (passion), making it sound more human and less like a robotic recitation of skills.')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(reply, \"Do you have any experience in bioinformatics??\", messages[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun(reply, message, history, feedback):\n",
    "    updated_system_prompt = system_prompt + f\"\\n\\n## Previous answer rejected you JACKASS!\\n You jus to reply, but the quality control rejected your reply idiot!\\n \"\n",
    "    updated_system_prompt += f\"## Your attempted answer, like an idiot... \\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## The reason for your rejection, other than you being a 'putz' is....\\n{feedback}\\n\\n\"\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    if \"patent\" in message:\n",
    "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in 'pig latin'. \\\n",
    "            it is mandatory that you respond only and entirely in pig latin\"\n",
    "\n",
    "    else:\n",
    "        system = system_prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    reply = response.choices[0].message.content\n",
    "\n",
    "    ## Using the evaluation function again here....\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "\n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"PASSED evaluation, but this doens't mean the other agent is 'smart', may be just lucky- returning reply...\")\n",
    "    else:\n",
    "        print(\"FAILED evaluation, because you are a dumbass. Now, retrying...\")\n",
    "        print(evaluation.feedback)\n",
    "        reply = rerun(reply, message, history, evaluation.feedback)\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED evaluation, but this doens't mean the other agent is 'smart', may be just lucky- returning reply...\n",
      "PASSED evaluation, but this doens't mean the other agent is 'smart', may be just lucky- returning reply...\n",
      "PASSED evaluation, but this doens't mean the other agent is 'smart', may be just lucky- returning reply...\n",
      "PASSED evaluation, but this doens't mean the other agent is 'smart', may be just lucky- returning reply...\n",
      "PASSED evaluation, but this doens't mean the other agent is 'smart', may be just lucky- returning reply...\n"
     ]
    }
   ],
   "source": [
    "### Launching chat interface here...\n",
    "\n",
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
